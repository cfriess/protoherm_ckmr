
executor: cluster-generic
cluster-generic-submit-cmd:
  mkdir -p results/slurm_logs/{rule} &&
  sbatch
    --partition=medmem,himem
    --exclude=node[33-36],himem[04] 
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --time={resources.time}
    --job-name=smk-{rule}-{wildcards}
    --output=results/slurm_logs/{rule}/{rule}-%j-{wildcards}.out
    --error=results/slurm_logs/{rule}/{rule}-%j-{wildcards}.err
    --parsable
cluster-generic-status-cmd: status-sacct-robust.sh
cluster-generic-cancel-cmd: scancel
cluster-generic-cancel-nargs: 400
default-resources:
  - time="24:00:00"
  - mem_mb=4800
restart-times: 0
max-jobs-per-second: 10
max-status-checks-per-second: 25
local-cores: 1
latency-wait: 60
cores: 2400
jobs: 950
keep-going: True
rerun-incomplete: True
printshellcmds: True
software-deployment-method: conda
rerun-trigger: mtime

set-resources:
  - make_pedigree:mem_mb=24000
  - sample_pop:mem_mb=14000

